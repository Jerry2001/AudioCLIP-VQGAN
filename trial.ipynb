{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally made by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
    "# The original BigGAN+CLIP method was by https://twitter.com/advadnoun\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('taming-transformers')\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models import cond_transformer, vqgan\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.cuda import get_device_properties\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from torch_optimizer import DiffGrad, AdamP\n",
    "\n",
    "from CLIP import clip\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from PIL import ImageFile, Image, PngImagePlugin, ImageChops\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import re\n",
    "\n",
    "# Supress warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AudioCLIP\n",
    "\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import simplejpeg\n",
    "\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append('AudioCLIP')\n",
    "\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Full-Training.pt'\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 44100\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "\n",
    "# For zoom video\n",
    "def zoom_at(img, x, y, zoom):\n",
    "    w, h = img.size\n",
    "    zoom2 = zoom * 2\n",
    "    img = img.crop((x - w / zoom2, y - h / zoom2, \n",
    "                    x + w / zoom2, y + h / zoom2))\n",
    "    return img.resize((w, h), Image.LANCZOS)\n",
    "\n",
    "\n",
    "# NR: Testing with different intital images\n",
    "def random_noise_image(w,h):\n",
    "    random_image = Image.fromarray(np.random.randint(0,255,(w,h,3),dtype=np.dtype('uint8')))\n",
    "    return random_image\n",
    "\n",
    "\n",
    "# create initial gradient image\n",
    "def gradient_2d(start, stop, width, height, is_horizontal):\n",
    "    if is_horizontal:\n",
    "        return np.tile(np.linspace(start, stop, width), (height, 1))\n",
    "    else:\n",
    "        return np.tile(np.linspace(start, stop, height), (width, 1)).T\n",
    "\n",
    "\n",
    "def gradient_3d(width, height, start_list, stop_list, is_horizontal_list):\n",
    "    result = np.zeros((height, width, len(start_list)), dtype=float)\n",
    "\n",
    "    for i, (start, stop, is_horizontal) in enumerate(zip(start_list, stop_list, is_horizontal_list)):\n",
    "        result[:, :, i] = gradient_2d(start, stop, width, height, is_horizontal)\n",
    "\n",
    "    return result\n",
    "\n",
    "    \n",
    "def random_gradient_image(w,h):\n",
    "    array = gradient_3d(w, h, (0, 0, np.random.randint(0,255)), (np.random.randint(1,255), np.random.randint(2,255), np.random.randint(3,128)), (True, False, False))\n",
    "    random_image = Image.fromarray(np.uint8(array))\n",
    "    return random_image\n",
    "\n",
    "\n",
    "# Used in older MakeCutouts\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.view([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.view([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "\n",
    "\n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    "\n",
    "replace_grad = ReplaceGrad.apply\n",
    "\n",
    "\n",
    "class ClampWithGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, min, max):\n",
    "        ctx.min = min\n",
    "        ctx.max = max\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min, max)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
    "\n",
    "clamp_with_grad = ClampWithGrad.apply\n",
    "\n",
    "\n",
    "def vector_quantize(x, codebook):\n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)\n",
    "\n",
    "\n",
    "class Prompt(nn.Module):\n",
    "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('weight', torch.as_tensor(weight))\n",
    "        self.register_buffer('stop', torch.as_tensor(stop))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
    "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        dists = dists * self.weight.sign()\n",
    "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
    "\n",
    "\n",
    "#NR: Split prompts and weights\n",
    "def split_prompt(prompt):\n",
    "    vals = prompt.rsplit(':', 2)\n",
    "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
    "    return vals[0], float(vals[1]), float(vals[2])\n",
    "\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow # not used with pooling\n",
    "        \n",
    "        # Pick your own augments & their order\n",
    "        augment_list = []\n",
    "        for item in []:\n",
    "            if item == 'Ji':\n",
    "                augment_list.append(K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.7))\n",
    "            elif item == 'Sh':\n",
    "                augment_list.append(K.RandomSharpness(sharpness=0.3, p=0.5))\n",
    "            elif item == 'Gn':\n",
    "                augment_list.append(K.RandomGaussianNoise(mean=0.0, std=1., p=0.5))\n",
    "            elif item == 'Pe':\n",
    "                augment_list.append(K.RandomPerspective(distortion_scale=0.7, p=0.7))\n",
    "            elif item == 'Ro':\n",
    "                augment_list.append(K.RandomRotation(degrees=15, p=0.7))\n",
    "            elif item == 'Af':\n",
    "                augment_list.append(K.RandomAffine(degrees=15, translate=0.1, shear=5, p=0.7, padding_mode='zeros', keepdim=True)) # border, reflection, zeros\n",
    "            elif item == 'Et':\n",
    "                augment_list.append(K.RandomElasticTransform(p=0.7))\n",
    "            elif item == 'Ts':\n",
    "                augment_list.append(K.RandomThinPlateSpline(scale=0.8, same_on_batch=True, p=0.7))\n",
    "            elif item == 'Cr':\n",
    "                augment_list.append(K.RandomCrop(size=(self.cut_size,self.cut_size), pad_if_needed=True, padding_mode='reflect', p=0.5))\n",
    "            elif item == 'Er':\n",
    "                augment_list.append(K.RandomErasing(scale=(.1, .4), ratio=(.3, 1/.3), same_on_batch=True, p=0.7))\n",
    "            elif item == 'Re':\n",
    "                augment_list.append(K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5))\n",
    "                \n",
    "        self.augs = nn.Sequential(*augment_list)\n",
    "        self.noise_fac = 0.1\n",
    "        # self.noise_fac = False\n",
    "\n",
    "        # Uncomment if you like seeing the list ;)\n",
    "        # print(augment_list)\n",
    "        \n",
    "        # Pooling\n",
    "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        cutouts = []\n",
    "        \n",
    "        for _ in range(self.cutn):            \n",
    "            # Use Pooling\n",
    "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
    "            cutouts.append(cutout)\n",
    "            \n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        \n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# An updated version with Kornia augments and pooling (where my version started):\n",
    "class MakeCutoutsPoolingUpdate(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow # Not used with pooling\n",
    "\n",
    "        self.augs = nn.Sequential(\n",
    "            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
    "            K.RandomPerspective(0.7,p=0.7),\n",
    "            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
    "            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),            \n",
    "        )\n",
    "        \n",
    "        self.noise_fac = 0.1\n",
    "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        \n",
    "        for _ in range(self.cutn):\n",
    "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
    "            cutouts.append(cutout)\n",
    "            \n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        \n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# An Nerdy updated version with selectable Kornia augments, but no pooling:\n",
    "class MakeCutoutsNRUpdate(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "        self.noise_fac = 0.1\n",
    "        \n",
    "        # Pick your own augments & their order\n",
    "        augment_list = []\n",
    "        for item in []:\n",
    "            if item == 'Ji':\n",
    "                augment_list.append(K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.7))\n",
    "            elif item == 'Sh':\n",
    "                augment_list.append(K.RandomSharpness(sharpness=0.3, p=0.5))\n",
    "            elif item == 'Gn':\n",
    "                augment_list.append(K.RandomGaussianNoise(mean=0.0, std=1., p=0.5))\n",
    "            elif item == 'Pe':\n",
    "                augment_list.append(K.RandomPerspective(distortion_scale=0.5, p=0.7))\n",
    "            elif item == 'Ro':\n",
    "                augment_list.append(K.RandomRotation(degrees=15, p=0.7))\n",
    "            elif item == 'Af':\n",
    "                augment_list.append(K.RandomAffine(degrees=30, translate=0.1, shear=5, p=0.7, padding_mode='zeros', keepdim=True)) # border, reflection, zeros\n",
    "            elif item == 'Et':\n",
    "                augment_list.append(K.RandomElasticTransform(p=0.7))\n",
    "            elif item == 'Ts':\n",
    "                augment_list.append(K.RandomThinPlateSpline(scale=0.8, same_on_batch=True, p=0.7))\n",
    "            elif item == 'Cr':\n",
    "                augment_list.append(K.RandomCrop(size=(self.cut_size,self.cut_size), pad_if_needed=True, padding_mode='reflect', p=0.5))\n",
    "            elif item == 'Er':\n",
    "                augment_list.append(K.RandomErasing(scale=(.1, .4), ratio=(.3, 1/.3), same_on_batch=True, p=0.7))\n",
    "            elif item == 'Re':\n",
    "                augment_list.append(K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5))\n",
    "                \n",
    "        self.augs = nn.Sequential(*augment_list)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# An updated version with Kornia augments, but no pooling:\n",
    "class MakeCutoutsUpdate(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "        self.augs = nn.Sequential(\n",
    "            K.RandomHorizontalFlip(p=0.5),\n",
    "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
    "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
    "            K.RandomSharpness(0.3,p=0.4),\n",
    "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
    "            K.RandomPerspective(0.2,p=0.4),)\n",
    "        self.noise_fac = 0.1\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# This is the original version (No pooling)\n",
    "class MakeCutoutsOrig(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "        return clamp_with_grad(torch.cat(cutouts, dim=0), 0, 1)\n",
    "\n",
    "\n",
    "def load_vqgan_model(config_path, checkpoint_path):\n",
    "    global gumbel\n",
    "    gumbel = False\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
    "        model = vqgan.VQModel(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
    "        model = vqgan.GumbelVQ(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "        gumbel = True\n",
    "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
    "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
    "        parent_model.eval().requires_grad_(False)\n",
    "        parent_model.init_from_ckpt(checkpoint_path)\n",
    "        model = parent_model.first_stage_model\n",
    "    else:\n",
    "        raise ValueError(f'unknown model type: {config.model.target}')\n",
    "    del model.loss\n",
    "    return model\n",
    "\n",
    "\n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = \"cat.wav\"\n",
    "iterations = 500\n",
    "clip_model = 'ViT-B/32'\n",
    "vqgan_config = f'checkpoints/vqgan_imagenet_f16_16384.yaml'\n",
    "vqgan_checkpoint = f'checkpoints/vqgan_imagenet_f16_16384.ckpt'\n",
    "seed = None\n",
    "optimiser = 'Adam'\n",
    "output = \"output.png\"\n",
    "cuda_device = \"cuda:0\"\n",
    "learning_rate = 0.1\n",
    "cut_power = 1.0\n",
    "cutn = 32\n",
    "size = [256, 256]\n",
    "display_freq = 50\n",
    "prompt = \"A cute, smiling, Nerdy Rodent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Restored from checkpoints/vqgan_imagenet_f16_16384.ckpt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(cuda_device)\n",
    "model = load_vqgan_model(vqgan_config, vqgan_checkpoint).to(device)\n",
    "jit = True if \"1.7.1\" in torch.__version__ else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptor = clip.load(clip_model, jit=jit)[0].eval().requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_size = perceptor.visual.input_resolution\n",
    "f = 2**(model.decoder.num_resolutions - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cutouts = MakeCutouts(cut_size, cutn, cut_pow=cut_power)\n",
    "\n",
    "toksX, toksY = size[0] // f, size[1] // f\n",
    "sideX, sideY = toksX * f, toksY * f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_dim = model.quantize.e_dim\n",
    "n_toks = model.quantize.n_e\n",
    "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
    "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.3643e-05, -2.6730e-06, -1.2559e-05,  ..., -1.7800e-05,\n",
       "            4.0697e-05,  5.0808e-05],\n",
       "          [-3.6812e-05, -3.2564e-05,  6.2193e-06,  ...,  3.8076e-05,\n",
       "           -5.3375e-05, -1.8076e-06],\n",
       "          [ 2.7826e-05,  2.1402e-05, -2.9303e-05,  ..., -4.2843e-05,\n",
       "            1.1637e-05, -8.0978e-01],\n",
       "          ...,\n",
       "          [ 4.3127e-05,  2.3784e-05, -1.8613e-05,  ..., -4.1346e-06,\n",
       "            4.3580e-06, -5.0171e-05],\n",
       "          [-1.5357e-05, -1.9096e-05, -2.1002e-05,  ..., -3.1081e-05,\n",
       "           -3.6478e-05, -2.1435e-05],\n",
       "          [-2.7652e-05, -2.8008e-05,  3.0875e-05,  ...,  4.9766e-01,\n",
       "           -2.8779e-06,  4.9798e-06]],\n",
       "\n",
       "         [[ 4.3286e-06,  6.1450e-06,  5.4384e-05,  ..., -5.9849e-05,\n",
       "            5.3639e-05,  1.5825e-05],\n",
       "          [-3.9289e-05, -4.9387e-05, -5.6914e-05,  ..., -3.5552e-05,\n",
       "            5.1115e-05, -8.1304e-06],\n",
       "          [ 3.8624e-06,  3.7811e-05, -2.9863e-05,  ..., -3.7606e-05,\n",
       "            1.5771e-05, -1.3476e-01],\n",
       "          ...,\n",
       "          [-5.0518e-05, -1.6808e-05, -3.9576e-05,  ...,  5.4021e-05,\n",
       "           -1.2956e-05, -5.2995e-05],\n",
       "          [-5.4956e-05, -5.7852e-05,  6.6692e-06,  ...,  1.0816e-05,\n",
       "            1.7817e-05,  2.1178e-05],\n",
       "          [-8.8520e-06, -2.6445e-05, -5.9309e-05,  ..., -5.4210e-01,\n",
       "           -1.6130e-06, -2.3699e-05]],\n",
       "\n",
       "         [[ 9.6912e-06, -5.0656e-05, -4.6619e-05,  ..., -5.1421e-05,\n",
       "            5.4568e-05,  1.6599e-05],\n",
       "          [ 4.2933e-05,  6.0514e-05,  2.5430e-05,  ..., -1.0027e-05,\n",
       "           -1.9711e-06,  4.3655e-05],\n",
       "          [ 1.1497e-05, -2.4017e-05, -1.6191e-05,  ..., -1.4769e-06,\n",
       "            5.3653e-05,  1.0029e+00],\n",
       "          ...,\n",
       "          [-5.1031e-05,  4.9841e-05, -1.2125e-05,  ...,  5.0537e-05,\n",
       "           -4.6050e-05, -4.9555e-05],\n",
       "          [ 5.4895e-05, -2.9735e-05, -2.1822e-06,  ..., -3.7514e-05,\n",
       "            4.2832e-06, -1.4223e-05],\n",
       "          [-1.9078e-05, -5.4431e-05, -2.6140e-05,  ..., -1.7250e+00,\n",
       "           -5.3566e-06, -3.6850e-05]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.8829e-05,  2.3655e-05, -4.2382e-06,  ..., -9.0342e-06,\n",
       "           -1.5078e-07,  2.6336e-05],\n",
       "          [-2.7888e-05, -8.3851e-06, -9.1609e-06,  ..., -4.3645e-05,\n",
       "           -4.4171e-05,  2.3664e-05],\n",
       "          [ 4.5160e-05,  5.7948e-05, -1.0194e-05,  ..., -2.9388e-05,\n",
       "           -3.8730e-05, -2.8833e-01],\n",
       "          ...,\n",
       "          [-5.7770e-05,  4.5776e-05,  1.4132e-05,  ...,  2.8598e-05,\n",
       "           -8.5068e-06,  1.7620e-06],\n",
       "          [ 2.4417e-05, -6.4992e-06,  1.1297e-05,  ...,  3.1892e-05,\n",
       "           -2.9648e-06,  1.7728e-05],\n",
       "          [-4.9855e-05,  4.3493e-05, -5.5164e-05,  ..., -5.4905e-01,\n",
       "           -4.7033e-05,  5.8709e-05]],\n",
       "\n",
       "         [[ 2.7828e-06,  9.1932e-06,  2.0858e-05,  ..., -5.1991e-05,\n",
       "            5.5307e-05,  5.3553e-05],\n",
       "          [-4.5477e-05, -2.0668e-05,  1.3413e-05,  ...,  4.5369e-05,\n",
       "           -4.5641e-05,  3.0375e-05],\n",
       "          [-5.7874e-05, -4.2067e-05, -3.2982e-05,  ...,  5.2960e-05,\n",
       "            3.6396e-05,  8.9604e-01],\n",
       "          ...,\n",
       "          [-3.2188e-05,  5.5809e-05, -2.3628e-06,  ...,  3.6653e-05,\n",
       "           -1.6437e-05, -2.8851e-05],\n",
       "          [ 1.5782e-05, -4.0660e-05, -9.5668e-06,  ..., -2.5807e-05,\n",
       "           -9.4456e-06, -7.2424e-06],\n",
       "          [ 1.8286e-05, -5.5441e-05,  2.2126e-05,  ..., -1.3081e+00,\n",
       "           -3.4079e-05,  4.1177e-05]],\n",
       "\n",
       "         [[ 5.8608e-05,  2.3546e-05, -3.3673e-05,  ...,  3.1334e-06,\n",
       "           -5.2429e-05, -3.9914e-05],\n",
       "          [-1.3822e-05,  5.3490e-05,  5.9871e-05,  ..., -2.9064e-05,\n",
       "            8.0457e-06,  5.0674e-05],\n",
       "          [-5.6720e-05, -4.7010e-05, -3.7228e-05,  ..., -4.6433e-05,\n",
       "            3.7182e-05, -5.1874e-01],\n",
       "          ...,\n",
       "          [-2.4212e-05, -8.5843e-06,  6.3725e-06,  ...,  1.0808e-06,\n",
       "            4.7625e-05, -4.7565e-05],\n",
       "          [-2.4207e-05,  3.5676e-05, -1.8628e-05,  ...,  4.6694e-05,\n",
       "           -2.2924e-05,  2.3307e-06],\n",
       "          [ 2.7648e-06,  2.3467e-05, -5.2193e-05,  ..., -2.4956e-02,\n",
       "           -3.1972e-05,  3.7758e-05]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Init Z\n",
    "one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
    "\n",
    "z = one_hot @ model.quantize.embedding.weight\n",
    "z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n",
    "\n",
    "z_orig = z.clone()\n",
    "z.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam([z], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A cute, smiling, Nerdy Rodent']\n"
     ]
    }
   ],
   "source": [
    "print(story_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15539/4051535042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstory_phrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make a list of all phrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_phrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstory_phrases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "story_phrases = [phrase.strip() for phrase in prompts.split(\"^\")]\n",
    "    \n",
    "# Make a list of all phrases\n",
    "all_phrases = []\n",
    "for phrase in story_phrases:\n",
    "    all_phrases.append(phrase.split(\"|\"))\n",
    "\n",
    "# First phrase\n",
    "prompts = all_phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pMs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_prompt(prompt):\n",
    "    vals = prompt.rsplit(':', 2)\n",
    "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
    "    return vals[0], float(vals[1]), float(vals[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    txt, weight, stop = split_prompt(prompt)\n",
    "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
    "    pMs.append(Prompt(embed, weight, stop).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth(z):\n",
    "    if gumbel:\n",
    "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
    "    else:\n",
    "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
    "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def checkin(i, losses):\n",
    "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
    "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
    "    out = synth(z)\n",
    "    info = PngImagePlugin.PngInfo()\n",
    "    TF.to_pil_image(out[0].cpu()).save(output, pnginfo=info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascend_txt():\n",
    "    global i\n",
    "    out = synth(z)\n",
    "    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for prompt in pMs:\n",
    "        result.append(prompt(iii))\n",
    "\n",
    "    return result # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    lossAll = ascend_txt()\n",
    "    \n",
    "    if i % display_freq == 0:\n",
    "        checkin(i, lossAll)\n",
    "       \n",
    "    loss = sum(lossAll)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        z.copy_(z.maximum(z_min).minimum(z_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                  std=[0.26862954, 0.26130258, 0.27577711])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it\n",
    "\n",
    "i = 0 # Iteration counter\n",
    "\n",
    "try:\n",
    "    with tqdm() as pbar:\n",
    "        while True:            \n",
    "            train(i)\n",
    "            if i == max_iterations:\n",
    "                break\n",
    "            i += 1\n",
    "            pbar.update()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-clip",
   "language": "python",
   "name": "audio-clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
