{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally made by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
    "# The original BigGAN+CLIP method was by https://twitter.com/advadnoun\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('taming-transformers')\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models import cond_transformer, vqgan\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.cuda import get_device_properties\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from torch_optimizer import DiffGrad, AdamP\n",
    "\n",
    "from CLIP import clip\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from PIL import ImageFile, Image, PngImagePlugin, ImageChops\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import re\n",
    "\n",
    "# Supress warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AudioCLIP\n",
    "\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import simplejpeg\n",
    "\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append('AudioCLIP')\n",
    "\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Full-Training.pt'\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 44100\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various functions and classes\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    "\n",
    "replace_grad = ReplaceGrad.apply\n",
    "\n",
    "\n",
    "class ClampWithGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, min, max):\n",
    "        ctx.min = min\n",
    "        ctx.max = max\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min, max)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
    "\n",
    "clamp_with_grad = ClampWithGrad.apply\n",
    "\n",
    "\n",
    "def vector_quantize(x, codebook):\n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)\n",
    "\n",
    "\n",
    "class Prompt(nn.Module):\n",
    "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('weight', torch.as_tensor(weight))\n",
    "        self.register_buffer('stop', torch.as_tensor(stop))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
    "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        dists = dists * self.weight.sign()\n",
    "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow # not used with pooling\n",
    "        \n",
    "        # Pick your own augments & their order\n",
    "        augment_list = []\n",
    "        #for item in args.augments[0]:\n",
    "        #    if item == 'Ji':\n",
    "        #        augment_list.append(K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.7))\n",
    "        #    elif item == 'Sh':\n",
    "        #        augment_list.append(K.RandomSharpness(sharpness=0.3, p=0.5))\n",
    "        #    elif item == 'Gn':\n",
    "        #       augment_list.append(K.RandomGaussianNoise(mean=0.0, std=1., p=0.5))\n",
    "        #    elif item == 'Pe':\n",
    "        #        augment_list.append(K.RandomPerspective(distortion_scale=0.7, p=0.7))\n",
    "        #    elif item == 'Ro':\n",
    "        #        augment_list.append(K.RandomRotation(degrees=15, p=0.7))\n",
    "        #    elif item == 'Af':\n",
    "        #        augment_list.append(K.RandomAffine(degrees=15, translate=0.1, shear=5, p=0.7, padding_mode='zeros', keepdim=True)) # border, reflection, zeros\n",
    "        #    elif item == 'Et':\n",
    "        #       augment_list.append(K.RandomElasticTransform(p=0.7))\n",
    "        #    elif item == 'Ts':\n",
    "        #        augment_list.append(K.RandomThinPlateSpline(scale=0.8, same_on_batch=True, p=0.7))\n",
    "        #    elif item == 'Cr':\n",
    "        #        augment_list.append(K.RandomCrop(size=(self.cut_size,self.cut_size), pad_if_needed=True, padding_mode='reflect', p=0.5))\n",
    "        #   elif item == 'Er':\n",
    "        #       augment_list.append(K.RandomErasing(scale=(.1, .4), ratio=(.3, 1/.3), same_on_batch=True, p=0.7))\n",
    "        #   elif item == 'Re':\n",
    "        #       augment_list.append(K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5))\n",
    "                \n",
    "        self.augs = nn.Sequential(*augment_list)\n",
    "        self.noise_fac = 0.1\n",
    "        # self.noise_fac = False\n",
    "\n",
    "        # Uncomment if you like seeing the list ;)\n",
    "        # print(augment_list)\n",
    "        \n",
    "        # Pooling\n",
    "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        cutouts = []\n",
    "        \n",
    "        for _ in range(self.cutn):            \n",
    "            # Use Pooling\n",
    "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
    "            cutouts.append(cutout)\n",
    "            \n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        \n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "def load_vqgan_model(config_path, checkpoint_path):\n",
    "    global gumbel\n",
    "    gumbel = False\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
    "        model = vqgan.VQModel(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
    "        model = vqgan.GumbelVQ(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "        gumbel = True\n",
    "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
    "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
    "        parent_model.eval().requires_grad_(False)\n",
    "        parent_model.init_from_ckpt(checkpoint_path)\n",
    "        model = parent_model.first_stage_model\n",
    "    else:\n",
    "        raise ValueError(f'unknown model type: {config.model.target}')\n",
    "    del model.loss\n",
    "    return model\n",
    "\n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = \"cat.wav\"\n",
    "iterations = 500\n",
    "clip_model = 'ViT-B/32'\n",
    "vqgan_config = f'checkpoints/vqgan_imagenet_f16_16384.yaml'\n",
    "vqgan_checkpoint = f'checkpoints/vqgan_imagenet_f16_16384.ckpt'\n",
    "seed = None\n",
    "optimiser = 'Adam'\n",
    "output = \"output.png\"\n",
    "cuda_device = \"cuda:0\"\n",
    "learning_rate = 0.1\n",
    "cut_power = 1.0\n",
    "cutn = 32\n",
    "size = [256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Downloading vgg_lpips model from https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1 to taming/modules/autoencoder/lpips/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8.19kB [00:00, 77.1kB/s]                                                                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Restored from checkpoints/vqgan_imagenet_f16_16384.ckpt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(cuda_device)\n",
    "model = load_vqgan_model(vqgan_config, vqgan_checkpoint).to(device)\n",
    "jit = True if \"1.7.1\" in torch.__version__ else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclp = AudioCLIP(pretrained=f'AudioCLIP/assets/{MODEL_FILENAME}').eval().requires_grad_(False)\n",
    "#aclp = AudioCLIP(pretrained=f'AudioCLIP/assets/{MODEL_FILENAME}').eval().requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_size = aclp.visual.input_resolution\n",
    "f = 2**(model.decoder.num_resolutions - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cutouts = MakeCutouts(cut_size, cutn, cut_pow=cut_power)\n",
    "\n",
    "toksX, toksY = size[0] // f, size[1] // f\n",
    "sideX, sideY = toksX * f, toksY * f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_dim = model.quantize.e_dim\n",
    "n_toks = model.quantize.n_e\n",
    "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
    "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-5.0860e-05, -4.4980e-05,  2.6465e-05,  ..., -5.1227e-05,\n",
       "            4.1564e-05,  5.0871e-05],\n",
       "          [ 4.1710e-05, -1.2218e-05,  1.2120e-05,  ..., -2.7556e-05,\n",
       "           -5.2967e-05, -3.1150e-05],\n",
       "          [ 4.9424e-05, -5.9704e-06,  2.6192e-05,  ...,  3.0036e-05,\n",
       "            1.1311e-05,  5.1552e-05],\n",
       "          ...,\n",
       "          [-4.8969e-05,  4.6556e-05, -3.1751e-05,  ...,  5.4569e-05,\n",
       "           -3.7636e-05, -1.1583e-05],\n",
       "          [ 4.9381e-05, -1.8835e-06, -1.3657e-05,  ...,  1.8833e-05,\n",
       "            5.6527e-06,  2.4225e-05],\n",
       "          [-2.4815e-05, -8.7205e-06,  4.8004e-05,  ...,  3.0839e-05,\n",
       "           -5.0628e-05, -5.8633e-01]],\n",
       "\n",
       "         [[-2.0137e-05, -1.9467e-06,  1.4610e-05,  ..., -5.1431e-05,\n",
       "            3.9960e-05,  3.4790e-05],\n",
       "          [-3.3092e-05,  4.8263e-05, -3.0485e-05,  ...,  4.2150e-05,\n",
       "            5.2267e-06, -1.6224e-06],\n",
       "          [ 2.8819e-05, -2.1620e-05,  2.2593e-05,  ..., -3.8716e-06,\n",
       "           -4.9852e-05, -3.3450e-05],\n",
       "          ...,\n",
       "          [-3.3196e-05, -1.6171e-05, -1.2214e-05,  ...,  5.1389e-05,\n",
       "           -2.9867e-05, -4.2759e-05],\n",
       "          [-5.8693e-05,  4.6268e-05,  2.5461e-05,  ..., -3.1970e-05,\n",
       "           -9.8349e-06, -2.6686e-05],\n",
       "          [ 1.0710e-05,  1.9145e-05,  7.6581e-06,  ..., -3.7743e-05,\n",
       "           -9.1447e-06,  1.5357e-01]],\n",
       "\n",
       "         [[-4.7362e-05,  5.4595e-05,  1.4225e-05,  ..., -2.8089e-05,\n",
       "           -5.1412e-05,  3.4934e-05],\n",
       "          [-4.7328e-05,  3.6962e-05,  4.4741e-06,  ..., -4.8886e-05,\n",
       "            4.5856e-05,  4.7402e-05],\n",
       "          [-2.6754e-05, -1.8460e-05, -1.6970e-05,  ...,  1.4910e-05,\n",
       "            9.3911e-08, -2.0482e-05],\n",
       "          ...,\n",
       "          [ 6.0813e-06, -2.6457e-05,  1.5065e-05,  ..., -1.2624e-05,\n",
       "           -4.4579e-05, -2.2253e-05],\n",
       "          [-7.5633e-06, -1.5158e-05, -4.5971e-05,  ...,  6.0680e-05,\n",
       "            5.5585e-05,  3.6193e-05],\n",
       "          [-3.7553e-05,  1.5637e-05,  2.8897e-05,  ...,  5.8816e-05,\n",
       "           -4.7503e-05,  6.2881e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.9615e-06,  5.6742e-05, -5.4024e-05,  ..., -1.4650e-05,\n",
       "           -3.0387e-05,  5.0590e-05],\n",
       "          [ 6.0005e-05, -2.9717e-05,  2.1742e-05,  ...,  7.3242e-07,\n",
       "           -5.6962e-05,  3.6812e-05],\n",
       "          [-2.0161e-05, -5.8508e-05,  4.5260e-06,  ...,  4.3450e-06,\n",
       "           -5.9402e-05,  6.0772e-05],\n",
       "          ...,\n",
       "          [ 3.6043e-06,  4.1281e-05,  3.3453e-06,  ...,  4.1878e-05,\n",
       "            4.6618e-05,  5.1893e-05],\n",
       "          [ 1.5781e-05, -5.5371e-05,  5.3521e-05,  ...,  2.6235e-06,\n",
       "           -1.9465e-05, -5.9844e-05],\n",
       "          [-5.2176e-05,  5.7437e-05,  4.6787e-05,  ...,  1.4965e-05,\n",
       "           -3.1950e-05, -1.0772e-01]],\n",
       "\n",
       "         [[-3.5983e-05,  3.2037e-05,  1.3601e-05,  ..., -1.9028e-05,\n",
       "            2.1726e-05,  5.7738e-05],\n",
       "          [ 1.0580e-05, -2.8394e-05,  1.5195e-05,  ..., -3.9735e-05,\n",
       "           -3.3664e-05,  1.5373e-05],\n",
       "          [ 4.9936e-05,  1.3789e-05,  3.9850e-05,  ...,  5.6053e-05,\n",
       "            6.0457e-05, -4.6961e-06],\n",
       "          ...,\n",
       "          [ 1.6864e-05, -2.2317e-05,  4.9603e-05,  ...,  4.0168e-05,\n",
       "           -3.1936e-05,  5.4001e-05],\n",
       "          [ 5.0446e-05, -4.1103e-05, -5.6697e-05,  ...,  5.8398e-05,\n",
       "            4.2159e-05, -6.0442e-05],\n",
       "          [ 3.9965e-05,  6.0945e-05, -1.0469e-05,  ...,  6.4772e-06,\n",
       "           -2.7410e-05,  5.2402e-01]],\n",
       "\n",
       "         [[-1.9834e-05,  2.9374e-05,  4.7640e-06,  ..., -5.5018e-06,\n",
       "           -1.5371e-05,  2.4294e-05],\n",
       "          [-2.9708e-05,  5.0938e-05, -7.1370e-06,  ..., -5.5498e-05,\n",
       "            2.7811e-05,  4.4608e-05],\n",
       "          [ 7.0309e-06, -2.2165e-05,  8.8193e-06,  ..., -6.0921e-05,\n",
       "           -6.6876e-06,  3.1127e-05],\n",
       "          ...,\n",
       "          [-4.5394e-05, -2.3391e-05, -4.1208e-05,  ...,  1.0871e-05,\n",
       "           -3.1216e-05, -5.6743e-05],\n",
       "          [-3.2523e-05,  3.8956e-05,  5.7743e-06,  ...,  5.9868e-05,\n",
       "            1.9326e-05, -4.0217e-05],\n",
       "          [-1.9404e-05, -3.1699e-06,  4.9807e-05,  ..., -3.1279e-05,\n",
       "            1.5480e-05, -6.1361e-01]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Init Z\n",
    "one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
    "\n",
    "z = one_hot @ model.quantize.embedding.weight\n",
    "z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n",
    "\n",
    "z_orig = z.clone()\n",
    "z.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam([z], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/ngop/.envs/audio-clip/lib/python3.7/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "audio_transforms = ToTensor1D()\n",
    "\n",
    "image_transforms = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Resize(IMAGE_SIZE, interpolation=Image.BICUBIC),\n",
    "    tv.transforms.CenterCrop(IMAGE_SIZE),\n",
    "    tv.transforms.Normalize(IMAGE_MEAN, IMAGE_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(img_dir, audio_dir):\n",
    "    img_dir += \"*.jpg\"\n",
    "    audio_dir += \"*.wav\"\n",
    "    \n",
    "    paths_to_audio = glob.glob(audio_dir)\n",
    "\n",
    "    audio = list()\n",
    "    for path_to_audio in paths_to_audio:\n",
    "        track, _ = librosa.load(path_to_audio, sr=SAMPLE_RATE, dtype=np.float32)\n",
    "        # compute spectrograms using trained audio-head (fbsp-layer of ESResNeXt)\n",
    "        # thus, the actual time-frequency representation will be visualized\n",
    "        spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)))\n",
    "        spec = np.ascontiguousarray(spec.numpy()).view(np.complex64)\n",
    "        pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
    "\n",
    "        audio.append((track, pow_spec))\n",
    "    \n",
    "    paths_to_images = glob.glob(img_dir)\n",
    "\n",
    "    images = list()\n",
    "    for path_to_image in paths_to_images:\n",
    "        with open(path_to_image, 'rb') as jpg:\n",
    "            image = simplejpeg.decode_jpeg(jpg.read())\n",
    "            images.append(image)\n",
    "    return images, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_audio(audio):\n",
    "    fig, axes = plt.subplots(2, len(audio), figsize=(20, 5), dpi=100)\n",
    "\n",
    "    for idx in range(len(audio)):\n",
    "        track, pow_spec = audio[idx]\n",
    "\n",
    "        # draw the waveform\n",
    "        librosa.display.waveplot(track, sr=SAMPLE_RATE, ax=axes[0, idx], color='k')\n",
    "        # show the corresponding power spectrogram\n",
    "        axes[1, idx].imshow(pow_spec, origin='lower', aspect='auto', cmap='gray', vmin=-180.0, vmax=20.0)\n",
    "\n",
    "        # modify legend\n",
    "        axes[0, idx].set_title(os.path.basename(paths_to_audio[idx]))\n",
    "        axes[0, idx].set_xlabel('')\n",
    "        axes[0, idx].set_xticklabels([])\n",
    "        axes[0, idx].grid(True)\n",
    "        axes[0, idx].set_ylim(bottom=-1, top=1)\n",
    "\n",
    "        axes[1, idx].set_xlabel('Time (s)')\n",
    "        axes[1, idx].set_xticks(np.linspace(0, pow_spec.shape[1], len(axes[0, idx].get_xticks())))\n",
    "        axes[1, idx].set_xticklabels([f'{tick:.1f}' if tick == int(tick) else '' for tick in axes[0, idx].get_xticks()])\n",
    "        axes[1, idx].set_yticks(np.linspace(0, pow_spec.shape[0] - 1, 5))\n",
    "\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    axes[1, 0].set_ylabel('Filter ID')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    for idx, path in enumerate(paths_to_audio):\n",
    "        print(os.path.basename(path))\n",
    "        display(Audio(audio[idx][0], rate=SAMPLE_RATE, embed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(text = None, images = None, audio = None):\n",
    "    text_features = []\n",
    "    image_features = []\n",
    "    audio_features = []\n",
    "    \n",
    "    if audio != None: \n",
    "        audio = torch.stack([audio_transforms(track.reshape(1, -1)) for track, _ in audio])\n",
    "        ((audio_features, _, _), _), _ = aclp(audio=audio)\n",
    "        audio_features = audio_features / torch.linalg.norm(audio_features, dim=-1, keepdim=True)\n",
    "    if images != None:\n",
    "        images = torch.stack([image_transforms(image) for image in images])\n",
    "        ((_, image_features, _), _), _ = aclp(image=images)\n",
    "        image_features = image_features / torch.linalg.norm(image_features, dim=-1, keepdim=True)\n",
    "    if text != None:\n",
    "        text = [[label] for label in text]\n",
    "        ((_, _, text_features), _), _ = aclp(text=text)\n",
    "        text_features = text_features / torch.linalg.norm(text_features, dim=-1, keepdim=True)\n",
    "    return text_features, image_features, audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, audio = read_data(\"AudioCLIP/demo/images/\", \"AudioCLIP/demo/sound/cutted/\")\n",
    "audio = [audio[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, audio_rep = embedding(audio = audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth(z):\n",
    "    if gumbel:\n",
    "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
    "    else:\n",
    "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
    "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def checkin(i, losses):\n",
    "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
    "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
    "    out = synth(z)\n",
    "    info = PngImagePlugin.PngInfo()\n",
    "    TF.to_pil_image(out[0].cpu()).save(output, pnginfo=info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascend_txt():\n",
    "    global i\n",
    "    out = synth(z)\n",
    "    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for prompt in pMs:\n",
    "        result.append(prompt(iii))\n",
    "\n",
    "    return result # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    lossAll = ascend_txt()\n",
    "    \n",
    "    if i % args.display_freq == 0:\n",
    "        checkin(i, lossAll)\n",
    "       \n",
    "    loss = sum(lossAll)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        z.copy_(z.maximum(z_min).minimum(z_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it\n",
    "\n",
    "i = 0 # Iteration counter\n",
    "\n",
    "try:\n",
    "    with tqdm() as pbar:\n",
    "        while True:            \n",
    "            train(i)\n",
    "            if i == args.max_iterations:\n",
    "                break\n",
    "            i += 1\n",
    "            pbar.update()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-clip",
   "language": "python",
   "name": "audio-clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
